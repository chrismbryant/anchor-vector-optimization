{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor-based 2-class word2vec classification\n",
    "\n",
    "* p anchors per class\n",
    "* k classes (k = 2)\n",
    "* n features (word2vec dimensions)\n",
    "* m examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google's Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec model loaded in 39 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(\"Word2vec model loaded in %d seconds.\" % int(time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "* Note the extreme repetition of examples in both the \"lodging\" and the \"other\" class . . . this may lead to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lodging\n",
    "data_lodging = pd.read_csv(\"data/lodging.csv\").word_clean.tolist()\n",
    "data_lodging_list = [[x.strip(\"'\") for x in words.strip(\"[]\").split(\", \")] for words in data_lodging]\n",
    "flat_data_lodging_list = [item for sublist in data_lodging_list for item in sublist if item]\n",
    "data_lodging_set = set(flat_data_lodging_list)\n",
    "\n",
    "# Not Lodging\n",
    "data_other = pd.read_csv(\"data/not_lodging.csv\").word_clean.tolist()\n",
    "data_other_list = [[x.strip(\"'\") for x in words.strip(\"[]\").split(\", \")] for words in data_other]\n",
    "flat_data_other_list = [item for sublist in data_other_list for item in sublist if item]\n",
    "data_other_set = set(flat_data_other_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67500, 90766, 8361)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lodging_list), len(flat_data_lodging_list), len(data_lodging_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67500, 81580, 1659)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_other_list), len(flat_data_other_list), len(data_other_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed data in word2vec space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_matrix(model, word_list):\n",
    "    \n",
    "    word_vec_list = []\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        \n",
    "        try:\n",
    "            word_vec = model.get_vector(word_list[i].title())\n",
    "            word_vec_list.append(word_vec)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                word_vec = model.get_vector(word_list[i])\n",
    "                word_vec_list.append(word_vec)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    word_matrix = np.array(word_vec_list).T\n",
    "    \n",
    "    return word_matrix\n",
    "        \n",
    "\n",
    "X_other = words_to_matrix(w2v_model, flat_data_other_list)\n",
    "X_lodging = words_to_matrix(w2v_model, flat_data_lodging_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare X and Y matrices and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # number of classes\n",
    "\n",
    "labels_other = [0 for i in range(X_other.shape[1])]\n",
    "labels_lodging = [1 for i in range(X_lodging.shape[1])]\n",
    "\n",
    "Y_other = np.eye(k)[labels_other].T\n",
    "Y_lodging = np.eye(k)[labels_lodging].T\n",
    "\n",
    "X = np.concatenate([X_other, X_lodging], axis = 1)\n",
    "Y = np.concatenate([Y_other, Y_lodging], axis = 1)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "indices = np.array([i for i in range(X.shape[1])])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Train-Dev-Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "valid_frac = 1 - train_frac - valid_frac\n",
    "\n",
    "m = X.shape[1]\n",
    "m_train = int(train_frac * m)\n",
    "m_valid = int(valid_frac * m)\n",
    "m_test = 1 - m_train - m_valid\n",
    "\n",
    "ix_train = indices[:m_train] \n",
    "ix_valid = indices[m_train:m_train + m_valid]\n",
    "ix_test = indices[-m_test:]\n",
    "\n",
    "X_train = X[:, ix_train]\n",
    "X_valid = X[:, ix_valid]\n",
    "X_test = X[:, ix_test]\n",
    "\n",
    "Y_train = Y[:, ix_train]\n",
    "Y_valid = Y[:, ix_valid]\n",
    "Y_test = Y[:, ix_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, minibatch_size, seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    _, m = X.shape\n",
    "    \n",
    "    indices = np.array([i for i in range(m)])\n",
    "    \n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X = X[:, indices]\n",
    "    Y = Y[:, indices]\n",
    "    \n",
    "    minibatches = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < m:\n",
    "        \n",
    "        minibatches.append((X[:, i:i+minibatch_size], Y[:, i:i+minibatch_size]))\n",
    "        i += minibatch_size\n",
    "    \n",
    "    return minibatches\n",
    "\n",
    "def create_placeholders(n, k):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = [n, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape = [k, None], name = \"Y\")\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def initialize_A(A_words=None, model=None):\n",
    "\n",
    "    # A.shape = (k, p, n)\n",
    "    \n",
    "    if A_words:\n",
    "        \n",
    "        p_counts = [len(A_words[label]) for label in A_words]\n",
    "        assert len(set(p_counts)) == 1\n",
    "        assert model != None\n",
    "        \n",
    "        A = np.array([[model.get_vector(w) for w in A_words[label]] for label in A_words])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        A = None\n",
    "        \n",
    "    return A\n",
    "\n",
    "def initialize_tf_params(k, p, n, A_init=None, learn_A=True):\n",
    "    \n",
    "    # k classes\n",
    "    # p anchors\n",
    "    # n features\n",
    "    \n",
    "    if type(A_init) != np.ndarray:\n",
    "        A = tf.get_variable(\"A\", \n",
    "                            [k, p, n], \n",
    "                            dtype = tf.float32, \n",
    "                            initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    else:\n",
    "        if learn_A:\n",
    "            A = tf.get_variable(\"A\", \n",
    "                                dtype = tf.float32, \n",
    "                                initializer = tf.constant(A_init, dtype = tf.float32))\n",
    "        else:\n",
    "            A = tf.constant(A_init, name = \"A\", dtype = tf.float32)\n",
    "        \n",
    "    w = tf.get_variable(\"w\", [k, 1], dtype = tf.float32, initializer = tf.zeros_initializer())\n",
    "    b = tf.get_variable(\"b\", [k, 1], dtype = tf.float32, initializer = tf.zeros_initializer())\n",
    "    \n",
    "    params = {\n",
    "        \"A\": A,\n",
    "        \"w\": w,\n",
    "        \"b\": b\n",
    "    }\n",
    "                        \n",
    "    return params\n",
    "\n",
    "def forward_propagate(X, params):\n",
    "\n",
    "    # A.shape = (k, p, n)\n",
    "    # X.shape = (n, m)\n",
    "    # w.shape = (k, 1)\n",
    "    # b.shape = (k, 1)\n",
    "    \n",
    "    A = params[\"A\"]\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    k = tf.shape(A)[0]\n",
    "    p = tf.shape(A)[1]\n",
    "    m = tf.shape(X)[1]\n",
    "    \n",
    "    norm_A = tf.reshape(tf.norm(A, axis = 2), (k, p, 1))\n",
    "    norm_X = tf.reshape(tf.norm(X, axis = 0), (1, m))\n",
    "    norm = tf.tensordot(norm_A, norm_X, [[2], [0]])\n",
    "    \n",
    "    prod = tf.tensordot(A, X, [[2], [0]])\n",
    "    \n",
    "    sim = tf.divide(prod, norm) # shape = (k, p, m)\n",
    "    dist = 1 - sim\n",
    "    \n",
    "    D = tf.reshape(tf.reduce_prod(dist, axis = 1), (k, m)) # shape = (k, m)\n",
    "    H = tf.transpose(tf.nn.softmax(tf.transpose(w * D + b))) # shape = (k, m)\n",
    "    \n",
    "    return H\n",
    "\n",
    "def get_cost(Y, H, epsilon = 0.0001):\n",
    "    \n",
    "    # Y.shape = (k, m)\n",
    "    # H.shape = (k, m)\n",
    "    \n",
    "    m = tf.cast(tf.shape(Y)[1], \"float32\")\n",
    "    L = - tf.reduce_sum(Y * tf.log(tf.maximum(H, epsilon)), axis = 0)\n",
    "    cost = 1/m * tf.reduce_sum(L)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def get_A_reg(A, q=0, epsilon = 0.0001):\n",
    "    \n",
    "    # A.shape = (k, p, n)\n",
    "    \n",
    "    if q == 0:\n",
    "        return 0\n",
    "    \n",
    "    k = tf.shape(A)[0]\n",
    "    p = tf.shape(A)[1]\n",
    "    \n",
    "    norm_A = tf.reshape(tf.norm(A, axis = 2), (k, p, 1))\n",
    "    norm_A_T = tf.transpose(norm_A, perm = [0, 2, 1]) # shape = (k, 1, p)\n",
    "    norm = tf.tensordot(norm_A, norm_A_T, [[1, 2], [2, 1]]) # shape = (k, p, p)\n",
    "    \n",
    "    prod = tf.tensordot(A, tf.transpose(A, perm = [0, 2, 1]), [[1, 2], [2, 1]])\n",
    "    \n",
    "    sim = tf.divide(prod, norm)\n",
    "    dist = tf.maximum(1 - sim, epsilon)\n",
    "    \n",
    "    energy = 1/2 * tf.reduce_sum(q**2 / dist)\n",
    "    \n",
    "    return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define anchor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, \n",
    "          Y_train, \n",
    "          X_valid, \n",
    "          Y_valid, \n",
    "          A_init = None,\n",
    "          learn_A = True,\n",
    "          p = 10,\n",
    "          q = 0,\n",
    "          learning_rate = 0.0001,\n",
    "          num_epochs = 1500, \n",
    "          minibatch_size = 32, \n",
    "          print_cost = True):\n",
    "    \n",
    "    # X_train.shape = (n, m)\n",
    "    # Y_train.shape = (p, m)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    seed = 2\n",
    "    \n",
    "    (n, m) = X_train.shape\n",
    "    k = Y_train.shape[0]\n",
    "    J_train = []\n",
    "    J_valid = []\n",
    "    \n",
    "    X, Y = create_placeholders(n, k)\n",
    "    params = initialize_tf_params(k, p, n, A_init, learn_A)\n",
    "    H = forward_propagate(X, params)\n",
    "    \n",
    "    if learn_A:\n",
    "        J = get_cost(Y, H) + get_A_reg(params[\"A\"], q = q)\n",
    "    else:\n",
    "        J = get_cost(Y, H)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(J)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        tic = time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed += 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, J], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            if print_cost == True and epoch % 10 == 0:\n",
    "                dt = time() - tic\n",
    "                print (\"Cost after epoch %i: %f (time elapsed = %f s)\" % (epoch, epoch_cost, dt))\n",
    "                \n",
    "            if epoch % 1 == 0:\n",
    "                J_train.append(epoch_cost)\n",
    "                J_valid.append(sess.run(J, feed_dict = {X: X_test, Y: Y_test}))\n",
    "        \n",
    "        font = {\"family\": \"Century Gothic\", \"weight\": \"normal\", \"size\" : 14}\n",
    "        plt.rc(\"font\", **font)\n",
    "        fig = plt.figure(figsize = (10, 7))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.plot(np.squeeze(J_train), label = \"J_train\")\n",
    "        ax.plot(np.squeeze(J_valid), label = \"J_valid\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim([0, len(J_train)])\n",
    "        ax.set_ylabel(\"Cost\", fontdict = {\"fontsize\": 16, \"weight\": \"bold\"})\n",
    "        ax.set_xlabel(\"Iteration\", fontdict = {\"fontsize\": 16, \"weight\": \"bold\"})\n",
    "        ax.set_title(\"%d Anchors: Charge = %g, Learning Rate = %g\" % (p, q, learning_rate),\n",
    "                     fontdict = {\"fontsize\": 24, \"weight\": \"bold\"})\n",
    "        plt.show()\n",
    "        \n",
    "        params = sess.run(params)\n",
    "        H_train = sess.run(H, feed_dict = {X: X_train})\n",
    "        H_valid = sess.run(H, feed_dict = {X: X_valid})\n",
    "        \n",
    "        print(\"Training complete.\")\n",
    "        \n",
    "        return (params, H_train, H_valid, J_train, J_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize A, and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.152543 (time elapsed = 4.491215 s)\n",
      "Cost after epoch 10: 0.113782 (time elapsed = 50.827546 s)\n"
     ]
    }
   ],
   "source": [
    "k = Y_train.shape[0]\n",
    "p = 10\n",
    "n = X_train.shape[0]\n",
    "\n",
    "A_words = {\n",
    "    0: [\"shoe\", \"plant\", \"bird\", \"yellow\", \"the\", \"why\", \"computer\", \"angry\", \"yesterday\", \"bored\"],\n",
    "    1: [\"travel\", \"hotel\", \"resort\", \"airfare\", \"cruise\", \"plane\", \"vacation\", \"train\", \"inn\", \"Hilton\"]\n",
    "}\n",
    "\n",
    "A_init = initialize_A(A_words = A_words, model = w2v_model)\n",
    "\n",
    "result = model(X_train, \n",
    "               Y_train, \n",
    "               X_valid, \n",
    "               Y_valid, \n",
    "               A_init = None,\n",
    "               learn_A = True,\n",
    "               p = 5,\n",
    "               q = 0,\n",
    "               learning_rate = 0.1, # 0.001\n",
    "               num_epochs = 31, \n",
    "               minibatch_size = 32, \n",
    "               print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, H_train, H_valid, J_train, J_valid = result\n",
    "acc_train = sum(np.argmax(H_train, axis = 0) == np.argmax(Y_train, axis = 0))/Y_train.shape[1]\n",
    "acc_valid = sum(np.argmax(H_valid, axis = 0) == np.argmax(Y_valid, axis = 0))/Y_valid.shape[1]\n",
    "\n",
    "print(\"Train Accuracy: %f\" % acc_train)\n",
    "print(\"  Dev Accuracy: %f\" % acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Learned Anchor Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_words(A_matrix):\n",
    "\n",
    "    k, p, n = A_matrix.shape\n",
    "    A_words = {}\n",
    "\n",
    "    for label in range(k):\n",
    "        anchor_list = []\n",
    "        for anchor in range(p):\n",
    "            word = w2v_model.similar_by_vector(A_matrix[label, anchor, :])[0][0]\n",
    "            anchor_list.append(word)\n",
    "        A_words[label] = anchor_list\n",
    "        \n",
    "    return A_words\n",
    "\n",
    "learned_A_words = get_nearest_words(params[\"A\"])\n",
    "learned_A_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"w\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ROC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(probabilities, thresh=0.5):\n",
    "    probs = probabilities\n",
    "    predictions = (probs > thresh).astype(int)\n",
    "    return predictions\n",
    "\n",
    "def get_tpr_and_fpr(predictions, ground_truth, pos, neg):\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    tpr = np.sum(np.logical_and(predictions == 1, ground_truth == 1))/pos\n",
    "    fpr = np.sum(np.logical_and(predictions == 1, ground_truth == 0))/neg\n",
    "    return tpr, fpr\n",
    "\n",
    "def get_roc(probabilities, y, thresh_values):\n",
    "\n",
    "    y = np.array(y)\n",
    "    pos = np.sum(y == 1)\n",
    "    neg = np.sum(y == 0)\n",
    "\n",
    "    roc_tpr = []\n",
    "    roc_fpr = []\n",
    "\n",
    "    for v in thresh_values:\n",
    "        predictions = predict(probabilities, v)\n",
    "        tpr, fpr = get_tpr_and_fpr(predictions, y, pos, neg)\n",
    "        roc_tpr.append(tpr)\n",
    "        roc_fpr.append(fpr)\n",
    "\n",
    "    auc = -trapz(roc_tpr, roc_fpr)\n",
    "\n",
    "    roc = {\n",
    "        \"tpr\" : roc_tpr,\n",
    "        \"fpr\" : roc_fpr,\n",
    "        \"auc\" : auc\n",
    "    }\n",
    "\n",
    "    return roc\n",
    "\n",
    "def plot_roc(y, probs, N=500, title=None):\n",
    "  \n",
    "    thresh_values = np.arange(N)/N\n",
    "\n",
    "    assert len(probs) == len(y)\n",
    "\n",
    "    roc = get_roc(probs, y, thresh_values)\n",
    "\n",
    "    fig = plt.figure(figsize = (10, 7))\n",
    "    ax = fig.add_subplot(1, 1, 1, aspect = \"equal\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    cmap = \"RdYlBu\"\n",
    "    \n",
    "    ax.scatter(roc[\"fpr\"], \n",
    "           roc[\"tpr\"], \n",
    "           s = 100, \n",
    "           c = thresh_values, \n",
    "           cmap = cmap,\n",
    "           marker = '.', \n",
    "           edgecolors = 'none')\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k-')\n",
    "    norm = matplotlib.colors.Normalize(thresh_values[0], thresh_values[-1])\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap = cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ticks=np.linspace(0, 1, 11))\n",
    "    cbar.set_label(\"Threshold Value\")\n",
    "\n",
    "    ax.set_xlabel(\"FPR\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.show()\n",
    "    plt.gcf().clear()\n",
    "\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = H_train[1, :]\n",
    "y_train = Y_train[1, :]\n",
    "\n",
    "probs_valid = H_valid[1, :]\n",
    "y_valid = Y_valid[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_train = plot_roc(y_train, probs_train, title = \"ROC: Train\")\n",
    "roc_valid = plot_roc(y_valid, probs_valid, title = \"ROC: Valid\")\n",
    "\n",
    "print(\"Train AUC: %f\" % roc_train[\"auc\"])\n",
    "print(\"  Dev AUC: %f\" % roc_valid[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar([\"Purified_Drinking_Water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
